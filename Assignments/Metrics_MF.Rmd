---
title: "Intro. to Econometrics"
author: Philip Economides
date: "Winter 2022"
#date: "<br>`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{mathtools}
  - \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
  - \usepackage{amssymb}
output: 
  html_document:
    toc: false
    toc_depth: 3  
    number_sections: false
    theme: flatly
    highlight: tango  
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r Setup, include = F}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, leaflet, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel, rddtools, readxl, emoGG, dslabs, gapminder, extrafont, Ecdat, wooldridge, tidyverse, janitor, kableExtra, gridExtra, estimatr)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f
# Notes directory
dir_slides <- "~/Dropbox/Courses/"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 4,
  fig.width = 6,
  # dpi = 300,
  # cache = T,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  axis.text.x = element_text(size = 10),
  axis.text.y = element_text(size = 10),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title.x = element_text(angle = 0, vjust = 0.5),
  axis.title.y = element_text(angle = 90, vjust = 0.5),
  legend.position = "none",
  axis.line = element_line(color="black", size = .5)
)
```


> This masterfile is intended to keep track of ideas, assignments etc.



# {.tabset .tabset-fade .tabset-pills}


## A2 - Using tidyverse and ggplot2

> Due date: 24 January 2022
>
> The objective here is to get you comfortable with mutating dataframes, while applying some of the theory and have a little fun with your new visualization skills.

I will provide the intial portion of the code in which I generate a set of series for the class.
Your job is to put these items into a dataframe object, then conduct some transformations of the random variables before unleashing your new visualization skills.

The last portion of these exercises will be far more free-form and let you get creative with your submissions. 
Ideally it will be good practice for fleshing out potential data project ideas.



### Q1. Simulation (20. pts)

In this simulation of data, our variables can be defined as follows;

---

#### The data-generating process

$$
\begin{align}
  \text{X}\sim&\mathop{\text{Uniform}}(0, 10), \\
  \text{Y} =& \ \ 2 + 3\text{X} + \varepsilon, \ \\
  \text{Z} =& 20 - 2\text{Y} + \nu\\
\end{align}
$$

where $\varepsilon\sim\mathop{N}(0,15)$ and $\nu\sim\mathop{N}(0,10)$.

```{r, ssim_dgp}
library(pacman)
p_load(dplyr, ggplot2)
# Choose an observation count (number of rows)
n <- 100
set.seed(12345)
# Generate data in a tibble
  e = rnorm(n, sd = 30)
  v = rnorm(n, sd = 20)
  x = runif(n, min = 0, max = 10)
  y = 8 - 3*x + e
  z = 20 - 2*y + v
```

#### Questions

&nbsp;
&nbsp;

i. Run the code above to get started. Set up a dataframe in which you include random variables X, Y and Z. __(4. pts)__

&nbsp;

```{r, Q1-1}


```

&nbsp;

ii. 'Standardize' the X & Y variables. Include these two new random variables in your existing dataframe from question 1 (Hint: use the `mutate` function). Why would one normally choose to standarize a variable? __(8. pts)__

&nbsp;

```{r, Q1-2}

```

&nbsp;

iii. Present a density plot of X using `ggplot2`. Do the same for the standarized random variable based on X. What differences are most notable to you about the two series? __(8. pts)__

&nbsp;

```{r, Q1-3, warning=F}

```

&nbsp;

---

### Q2. Identifying Outliers (20. pts)

_This experience will have you identify mild and extreme outliers in a given dataset. To avoid doubt with respect to cherry picking specific outliers in order to curve a particular strong fitted relationship between variables, I often rely on a standardized rule of thumb, the IQR rule._

_IQR stands for interquartile range. Any values below the lower bound and greater than the higher bound of these two IQR-based amounts would be purged from the data._

$$
\begin{aligned}
\text{IQR } =& 3\text{rd} \text{ Quartile} - 1\text{st} \text{ Quartile}\\
\text{Lower Bound} \ =&  1\text{st} \text{ Quartile} - \lambda*IQR\\
\text{Upper Bound} \ =&  3\text{rd} \text{ Quartile} + \lambda*IQR
\end{aligned}
$$ 

_So what is $\lambda$? Consider it a scalar that determines how much we wish to rid the data of outliers. The lower its value, the stronger a purge we would commit. If you want to sacrifice more variation in your data and be more strict towards outliers, you would set $\lambda=1.5$ and purge mild outliers. If you wish to only focus on extreme outliers, you would instead use $\lambda=3$_

Let's try this new tool out! Download the data (dirty_deputies_v2.csv) provided on Canvas and follow the given steps below to clean the data of its __mild__ outliers.

&nbsp;
&nbsp;

#### Procedure

Step 1. Store your summary statistics for the 'refund_value' variable. __(5. pts)__

&nbsp;

```{r, 2-1}

# Enter in the directory to your csv file. If saved in the same folder just type dirty_deputies_v2.csv
# dirty_deputies_v2 <- read_csv(" ")

```

&nbsp;

Step 2. Identify your key lower and upper bounds for the given random variable in a mild setting. __(5. pts)__

&nbsp;

```{r, 2-2}


```

&nbsp;

Step 3. Filter out any outliers from the dataframe and save this dataframe as a separate new one. How many observations were dropped and what percentage was that of the original observation count? __(5. pts)__

&nbsp;

```{r, 2-3}


```

&nbsp;

Step 4. Visualize the random variable of interest from both datasets. What percentage of observations were dropped in this process? __(5. pts)__

&nbsp;

```{r, 2-4}


```

&nbsp;

__Extra Credit:__ Are there potential issues with this approach for identifying outliers? List them below and provide supportive code.

```{r, 2-5}


```

Present your findings and comment repeat them in an extreme outlier case.
You may be wondering when is best to exclude outliers.
The general rule of thumb is more variation is better variation, hence outliers can be extremely informative.
At the same time, this leads to outliers often driving the significance of particularly strong results.
If a data set suffers from data entry error, outlier detection can be a very quick way to identify values that would realistically be considered impossible.
If you cannot correct the value, the next best step is to remove it.
Additionally, if the observed value is not part of the population you are studying (i.e. farmland properties cropping up in a residential data exercise), you can legitimately remove the outliers.
However, if your outliers are a natural part of the population of interest, then you should make every effort possible to keep the observation contained in your analysis. 
Prepare results including and excluding outliers to see how sensitive your findings are to variation in these potentially key observations. 


---

### Q3. Visualization (10. pts)

Find some interesting data online and attempt to visualize it in such a manner that you can draw out one interesting fact.
For example, supposing you had savings data across US households, you could comment on how the percentage of income saved doubled during the initial COVID period through a bar chart or line plot. 

Have fun with this one, it does not need to be perfect but it will be good practice for the data project. 

&nbsp;

```{r, 3-1}

```



## A3 - Regressions

> Due date: 31 January 2022


### Q1. Regression and Curvature (20. pts)

1. Load your packages and import the "nlsy79.csv" data. 
```{r, 3-1-1, echo=FALSE, message=FALSE}

# Echo=FALSE allows you to hide the code while showing its resulting output. In this case it will evaluate the code without showing this portion of the code in your resulting html file. 
# If you wanted to prevent an item of code from running, use EVAL=F or EVAL=FALSE (same effect).

# Packages
library(pacman)
p_load(tidyverse,ggplot2, stargazer)

# Here is how I do it through a work directory
#nlsy79 <- read_csv("C:/Users/phili/Desktop/Teaching/Problem-Sets/03/nlsy79.csv")

# Import yours but make sure the file is already in the correct work directory rather than using the guady location I've listed

###
###
###

```


The data could be described as follows:

```{r,  echo=FALSE}
Name = c("CASEID", "earn2009", "hgc", "race", "sex", "bmonth", "byear",
         "afqt", "region_1979", "faminc1978", "nsibs79")

Description = c("Unique identifier", "Earnings in 2009", "Years of education", "Race and Ethnicity", "Gender", "Birth Month", "Birth Year", "Armed Forces Qualifying Test Percentile",
                "Region", "Family Income in 1978", "Number of Siblings")

data_descrip = cbind(Name, Description)

as_tibble(data_descrip)
```
2. Regress earnings in 2009 on years of education. How much do earnings increase (Level & %) on average for every additional year of schooling? (Note: To place a variable into a % interpretation, you need to log transform your data. First start by filtering out zero and missing values.)


```{r, 3-1-2}

```

3. Present your regression outputs using `stargazer()` or `export_summs()`.

```{r, 3-1-3}

```

4. Use ggplot2 to plot the conditional expectation of earnings with respect to years of education. In other words, solve for the average earnings conditional on a given year of earnings across all your discrete values. (Y-axis = Earnings, X-axis=Years of Education).

Do you think it is reasonable to assume earnings increase linearly with years of schooling?

```{r, 3-1-4}

```


5. Generate a variable that equals years of education squared. Regress earnings (level, %) on years of education and years of education squared.  **Bonus:** How much do earnings increase for someone who gets 10 instead of 9 years of schooling? What about someone who gets 17 instead of 16?

6. Generate a new variable that equals years of education but coded as a factor variable (Note: Using mutate, the function for transforming this numeric vector of values into a vector of factors is simply `factor().

Now regress earnings on this factor variable without a constant. To remove the constant, type “-1” after specifying the variables in the regression (e.g. lm(Y ~ X - 1, data=best_data_ever)). **Bonus**:Comment on the results. Have we seen these results before?

```{r, 3-1-5}

## Here is another example of some piping of functions. In this case we've log transformed a variable. To do two, just include a comma and work away
### dataframe <- dataframe %>% mutate(random_variable_logged = log(random_variable),
#                                      double_variable        = 2*random_variable,
#                                      half_variable          = 0.5*random_variable,
#                                      combo             = ran_var_1 + ran_var_2)

# Feel free to delete the above when you answer, just a reminder/shortcut for making multiple variables. 

```

---

### Q2. Working with Residuals (20. pts)

Consider the data we simulated last week. In this case, I've changed the sample size $n$ from 100 to 1000. 

```{r, ssim_dgp_II}
library(pacman)
p_load(dplyr, ggplot2)
# Choose an observation count (number of rows)
n <- 1000
set.seed(1245)
# Generate data in a tibble
data_sim = tibble(
  e = rnorm(n, sd = 30),
  v = rnorm(n, sd = 20),
  x = runif(n, min = 0, max = 10),
  y = 8 - 3*x + e,
  z = 20 - 0.3*y + 3*x + v
)
```

We know all the important parameters of interest since we defined them. 
Pretend for a moment that we did not know them and drew this provided sample of the three simulated random variables, ${X,Y,Z}$, to generate regression estimates. 

1. Notice in this case that $x$ is correlated both with $y$ and $z$. What would happen if we omitted $x$ and used only $y$ to explain the variation in $z$?

Prepare a regression of the following models and discuss the impact of omitting $X_i$.

$$
\begin{aligned}
\text{Model 1:} \ \ Z_i =& \  \beta_1 + \beta_2 Y_i + u_i\\
\text{Model 2:} \ \ Z_i =& \ \gamma_1 + \gamma_2 Y_i + \gamma_3 X_i + \mu_i
\end{aligned}
$$

```{r, 3-2-1}

```


2. Using summary() to store your regression outputs, each object contains a vector of the corresponding 'errors' or 'residuals' associated with each respective regression.

Create a pair of objects, each being a vector of the residual values. Use cbind() or tibble() to combine these two variables into a single dataframe. 

Find the corresponding RSS by squaring each column of residuals and then take the colSums() of the dataframe. Print the two $RSS_j = \sum_{i=1}^n \hat{u}_{ij}^2$ values, where $j$ is a subscript to denote which model we are considering. 

```{r, 3-2-2}
# To store your residuals, first obtain them from the regression output using resid(), then use unname() to generate a series without the unnecessary row number across each item. 


```

$RSS_1=$ and $RSS_2=$. Given that RSS ____ is  _____, this must be the model that best represents the explanatory variation in $Z_i$. 


3. Use `geom_errorbar()` to compare your point estimates for $\beta_2$ and $\gamma_2$ described by models 1 and 2. You will need to store them and the ymax, ymin values in a single dataframe with two rows. 

```{r, 3-2-3}

```

Which one seems more confident? Which one is biased?

---

### Q3. Regression of Interest (10. pts)

Using your own set of data, describe the relationship between any two random variables, $X$ and $Y$s, using a regression model equation. 

Next, run the associated regression using your sample of data. Present the results using your preferred function for regression outputs (try to avoid sing the `summary()` function. It is quite detracting from otherwise professional looking work).

&nbsp;

```{r, 2-3-1}

# Load data/packages

# Prepare data

# Run your regression of interest

# Plot the variables on a scatterplot (geom_point) and add another layer using + geom_smooth() to include a fitted line. ?geom_smooth to see its required arguments. 

```


What kind of inferences you can draw out from observing the coefficient of interest?

Given your regression above, what inferences can be drawn from the intercept and $R^2$ estimates?

Could you suggest any confounding variables that may be biasing your results? They must be variables correlated with both of your choices of $X$ and $Y$. 

---


## A4 - Inference and Confidence Intervals

> Due date: 18 February 2022



### Q1. Gun Violence (Wrangling Practice)

The dataset Guns.csv contains data on 50 US states plus the District of Columbia
for the 23 years 1977-1999.
The main variables of interest are vio (the violent crime rate) and shall, a dummy variable for whether the state had a 'shall-carry' law in effect that year (i.e. a law instructing local authorities to issue concealed weapons permits to all
citizens, with some restrictions).
Recall that a dummy variable is any binary variable, and will take a value of  1 only if a particular condition is met. In this, case the treatment of having this law in place being in effect for a given state at a given time.
The data set includes other variables, for a description see the definitions table posted on Canvas. 

Follow the instructions below to prepare an analysis between these two random variables.


1. Import the Guns.csv file using `read_csv()` __(2. pts)__

```{r, 4-1-1}


```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

2. Run the following code once you have loaded your data. This will mutate your data to include the state names by creating a new dataset and then forming a merge. You will need to replace the object name with what you chose to name your data object. __(2. pts)__


```{r, 4-1-2}

# matching_obj <- tibble(
#  stateid = unique(Guns$stateid),
#  statename = c(state.name[1:8], "Dis. of Columbia", state.name[9:50])
#)

# To match all your names to the ids, we'll use dplyr's `left_join()`, any matching variable names will be assessed as the details to match on. When stateid 1 is detected across multiple rows, it will fill in "Alabama" under each for the new variable "statename".

# Guns <- left_join(Guns, matching_obj)

# As you can see it only used "stateid" from both objects to match observations from our right-hand data set "matching_obj" and join them into the left object "Guns"

# Use View(Guns) in the console to see how this worked out for your new statename variable

```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

3. Referring to your data object, summarise the average level of gun violence per statename into a separate data object using `group_by()` and `summarise()`. __(6. pts)__
  

```{r, 4-1-3}


```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

4. Present a visualization you consider appropriate for exposing these underlying state differences you generated in part 3. (Hint: `glimpse(data)` is a great way to choose which variables you need to focus on). __(6. pts)__

**Bonus:** Rather than having x-axis values determined by the alphabetical order of states, present your states in descending order (based on your average measure). Use the `reorder()` function inside your `aes()` to do so. 

```{r, 4-1-4}


```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

5. Provide comment on this distribution of gun violence, does anything strike you, from looking at these averages of the violent crime rate (incidents per 100,000 members of the population) over time, per state? Use some of the other variables in the dataset for gleaning further insight. __(4. pts)__

&nbsp;
&nbsp;
&nbsp;
&nbsp;


Note: These data were provided by Professor John Donohue of Stanford University and were used in his paper with Ian Ayres **“Shooting Down the ‘More Guns Less Crime’ Hypothesis” Stanford Law Review, 2003, Vol. 55, 1193-1312**. I've uploaded the paper on Canvas for those interested in reading about a fundamental application of econometric theory in a practical setting.

&nbsp;
&nbsp;
&nbsp;
&nbsp;

### Q2. Gun Violence (Regression and Inference)

Now that we have a better grasp of what our data is looking at, lets see how these "shall-carry law" laws affected average violent crime rates.

1. Regress the following relationship and store your estimates of our two population parameters using the `tidy()` function into an object called `reg_all`. __(4. pts)__ 

```{r, 4-2-1}



```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

2. Provide a stargazer table for these results. How would you interpret the regression results? Comment on the meaning behind our estimated coefficient and determine the statistical significance of this result. __(4. pts)__

```{r, 4-2-2}


```

&nbsp;
&nbsp;
&nbsp;
&nbsp;

3. Construct confidence intervals at the 99%, 95%, 90%, 80% and 70% level. Use `geom_errorbar()` to portray each of these confidence intervals in a single visualization (similar to our last slide when addressing inference). The appropriate $t^{crit}$ values to use are $\{2.576, 1.960, 1.645, 1.282, 1.036\}$, respectively. (Hint: That `reg_all` object you created here will be useful). __(4. pts)__

```{r, 4-2-3}


```


&nbsp;
&nbsp;
&nbsp;
&nbsp;

4. Supposing you were presenting regression results to your colleagues, how would you describe one of these intervals? __(4. pts)__


&nbsp;
&nbsp;
&nbsp;
&nbsp;


5. Inuitively, do you think any of our key assumptions regarding the Gauss-Markov theorem (assumptions 1 to 5) could have been violated when running this regression?
For each assumption you consider violated, provide an explanation for why this may be the case. (Hint: We'll be dealing with some of these problems when we go into multivariate regression model setting). __(4. pts)__

&nbsp;
&nbsp;
&nbsp;
&nbsp;


### Q3. Confidence Intervals (10. pts)

Find some interesting data online, potentially the sources you've settled on for your data project, and use your explanatory variable (X) to explain the variation in your dependent variable (Y), by applying an `lm()` regression. 

Present your results using `stargazer()` and then generate confidence intervals at the 70%, 80%, 90%, 95% and 99% confidence levels in a single graph. If your observations are less than 1000, you'll need to apply the correct critical values. 

See the t-table I've posted in this week's module with the homework for a reference point. Your degrees of freedom (df) is determined by $n-k$, your observation count minus your number of estimated parameters. Your two-tailed test score will be highlighted by the confidence scores along the bottom of the table.

---


## A5 - Simulating Endogeneity Problems

> Due date: 7 March 2022


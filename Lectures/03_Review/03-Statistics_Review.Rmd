---
title: "Statistics Review II"
subtitle: "EC 320: Introduction to Econometrics"
author: "Philip Economides"
date: "Winter 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr,
       latex2exp, parallel, Ecdat, wooldridge, dslabs, ggforce, DT)
# Define colors
red_pink <- "#e64173"
met_slate <- "#272822" 
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Notes directory
dir_slides <- "Lectures/03-Review/"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  warning = F,
  message = F
)
theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```


# Statistics Review

---
# Overview

A few key terms:

--

- .hi-pink[Population:] a (usually large) group of items or events we would like to know about. 

--

- .hi-pink[Parameter:] a value that describes that population. The parameter of interest is the parameter that the researcher seeks to learn about.

--

- .hi-pink[Sample:] a survey of a subset of the population. 

--

Usually we aim to draw observations .hi-blue[randomly] from the population, such that it becomes a .hi-blue[representative sample] of the population. More details on this later!

---
# Overview

__Focus:__ Populations vs Samples

- How can we make inferences about a .hi-pink[population] based on a small .hi-blue[sample] of the population?

- In particular, how do we learn about an unknown population _parameter_ of interest?

__Challenge:__ Usually cannot access information about the entire population.

__Solution:__ Sample from the population and estimate the parameter.

- Draw $n$ observations from the population, then use an estimator.

---
# Sampling

There are myriad ways to produce a sample,<sup>*</sup> but we will restrict our attention to __simple random sampling__, where

1. Each observation is a random variable.

2. The $n$ random variables are independent.

3. Life becomes much simpler for the econometrician.

.footnote[
<sup>*</sup> Only a subset of these can help produce reliable statistics.
]

---
# Estimators

An __estimator__ is a rule (or formula) for estimating an unknown population parameter given a sample of data.

--

- Each observation in the sample is a random variable.

--

- An estimator is a combination of random variables $\implies$ it is a random variable.

__Example:__ Sample mean

$$
\bar{X} = \dfrac{1}{n} \sum_{i=1}^n X_i
$$

- $\bar{X}$ is an estimator for the population mean $\mu$.

- Given a sample, $\bar{X}$ yields an __estimate__ $\bar{x}$ or $\hat{\mu}$, a specific number.

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 10
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 2, sd = 20),
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Means
m0 <- mean(pop_df$x)
m1 <- mean(subset(pop_df$x, pop_df$s1 == T))
m2 <- mean(subset(pop_df$x, pop_df$s2 == T))
m3 <- mean(subset(pop_df$x, pop_df$s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 1, X = 1:1e4, FUN = function(x, size = n_s) {
  pop_df %>% 
    sample_n(size = size) %>% 
    summarize(mu_hat = mean(x))
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "#195c23", size = 10) +
theme_empty
```

.center[**Population**]

]

--

.pull-right[

```{R, mean1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  theme_empty
```

.center[**Population relationship**]
<br>
$\mu = `r round(m0, 2)`$

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "#ffa600", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 1:** 10 random individuals]

]

--

.pull-right[

```{R, sample1 mean, echo = F, fig.fullwidth = T, dev = "svg", message=FALSE}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  geom_histogram(data = subset(pop_df, s1 == T), aes(x), fill = "#ffa600", alpha = 0.40) +
  geom_vline(xintercept = m1, size = 2, color = "#ffa600") +
  theme_empty
```

.center[

**Population relationship**
<br>
$\mu = `r round(m0, 2)`$

**Sample relationship**
<br>
$\hat{\mu} = `r round(m1, 2)`$

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "#ffa600", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 2:** 10 random individuals]

]

--

.pull-right[

```{R, sample2 mean, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  geom_histogram(data = subset(pop_df, s2 == T), aes(x), fill = "#ffa600", alpha = 0.50) +
  geom_vline(xintercept = m2, size = 2, color = "#ffa600") +
  theme_empty
```

.center[

**Population relationship**
<br>
$\mu = `r round(m0, 2)`$

**Sample relationship**
<br>
$\hat{\mu} = `r round(m2, 2)`$

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "#ffa600", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 3:** 10 random individuals]

]

--

.pull-right[

```{R, sample3 mean, echo = F, fig.fullwidth = T, dev = "svg", message=FALSE}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  geom_histogram(data = subset(pop_df, s3 == T), aes(x), fill = "#ffa600", alpha = 0.50) +
  geom_vline(xintercept = m3, size = 2, color = "#ffa600") +
  theme_empty
```

.center[

**Population relationship**
<br>
$\mu = `r round(m0, 2)`$

**Sample relationship**
<br>
$\hat{\mu} = `r round(m3, 2)`$

]

]

---
class: clear-slide, middle

Let's repeat this **10,000 times** and then plot the estimates.

(This exercise is called a Monte Carlo simulation.)

---
class: clear-slide, middle

```{R, simulation, echo = F, dev = "svg",  message=FALSE}
ggplot() +
  geom_histogram(data = sim_df, aes(mu_hat), fill = "#ffa600", alpha = 0.6) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  scale_x_continuous(breaks = m0, labels = TeX("$\\mu$")) +
  xlab(TeX("$\\hat{\\mu}$")) +
  theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title.x = element_text(size = 20, hjust = 1, color = met_slate),
      line = element_blank())
```

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[
```{R, simulation2, echo = F, dev = "svg", message=FALSE}
ggplot() +
  geom_histogram(data = sim_df, aes(mu_hat), fill = "#ffa600", alpha = 0.6) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  scale_x_continuous(breaks = m0, labels = TeX("$\\mu$")) +
  xlab(TeX("$\\hat{\\mu}$")) +
  theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title.x = element_text(size = 20, hjust = 1, color = met_slate),
      line = element_blank())
```
]

.pull-right[

- On average, the mean of the samples are close to the population mean.

- But...some individual samples can miss the mark.

- The difference between individual samples and the population creates __uncertainty__. 

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

**Answer:** Uncertainty matters.

- $\hat{\mu}$ is a random variable that depends on the sample.

- In practice, we don't know whether our sample is similar to the population or not. 

- Individual samples may have means that differ greatly from the population.

- We will have to keep track of this uncertainty.

---
# Properties of Estimators

Imagine that we want to estimate an unknown parameter $\mu$, and we know the distributions of three competing estimators. __Which one should we use?__ 

```{R, competing pdfs, echo = F, dev = "svg", fig.height = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = red_pink) +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_simple + 
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

--

**Answer 1: Unbiasedness.**

On average (after *many* samples), does the estimator tend toward the correct value?

**More formally:** Does the mean of estimator's distribution equal the parameter it estimates?

$$ \mathop{\text{Bias}_\mu} \left( \hat{\mu} \right) = \mathop{\mathbb{E}}\left[ \hat{\mu} \right] - \mu $$

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 1: Unbiasedness.**

.pull-left[

**Unbiased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] = \mu$

```{R, unbiased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

--

.pull-right[

**Biased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] \neq \mu$

```{R, biased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 2: Low Variance (a.k.a. Efficiency).**

The central tendencies (means) of competing distributions are not the only things that matter. We also care about the **variance** of an estimator.

$$ \mathop{\text{Var}} \left( \hat{\mu} \right) = \mathop{\mathbb{E}}\left[ \left( \hat{\mu} - \mathop{\mathbb{E}}\left[ \hat{\mu} \right] \right)^2 \right] $$

Lower variance estimators produce estimates closer to the mean in each sample.

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 2: Low Variance (a.k.a. Efficiency).**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# The Bias-Variance Tradeoff

Should we be willing to take a bit of bias to reduce the variance?

In econometrics, we generally prefer unbiased estimators. Some other disciplines think more about this tradeoff.

```{R, variance bias, echo = F, dev = "svg", fig.height = 4.5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# Unbiased Estimators

In addition to the sample mean, there are several other unbiased estimators we will use often.

- __Sample variance__ to estimate variance $\sigma^2$.

- __Sample covariance__ to estimate covariance $\sigma_{XY}$.

- __Sample correlation__ to estimate the population correlation coefficient $\rho_{XY}$.

---
# Unbiased Estimators

The sample variance $S_X^2$ is an unbiased estimator of the population variance $\sigma^2$:

$$S_{X}^2 = \dfrac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2.$$

---
# Unbiased Estimators

The sample covariance $S_{XY}$ is an unbiased estimator of the population covariance $\sigma_{XY}$:


$$S_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y}).$$

---
# Unbiased Estimators

The sample correlation $r_{XY}$ is an unbiased estimator of the population correlation coefficient $\rho_{XY}$:

$$r_{XY} = \dfrac{S_{XY}}{\sqrt{S_X^2} \sqrt{S_Y^2}}.$$

---
# Hypothesis Testing

Given What do we make of an estimate of the population mean?

- Is it meaningfully different than existing evidence on the population mean?
- Is is _statistically distinguishable_ from previously hypothesized values of the population mean?
- Is the estimate extreme enough to update our prior beliefs about the population mean?

We can conduct statistical tests to address these questions.

---
# Hypothesis Testing

__Null hypothesis (H.sub[0]):__ $\mu = \mu_0$

__Alternative hypothesis (H.sub[1]):__ $\mu \neq \mu_0$

--

There are four possible outcomes of our test:

1. We __fail to reject__ the null hypothesis and the null is true.

2. We __reject__ the null hypothesis and the null is false.

3. We __reject__ the null hypothesis, but the null is actually true (**Type I error**).

4. We __fail to reject__ the null hypothesis, but the null is actually false (**Type II error**).

---
# Hypothesis Testing

We __fail to reject__ the null hypothesis and the null is true.

- The defendant was acquitted and he didn't do the crime.

--

We __reject__ the null hypothesis and the null is false.

- The defendant was convicted and he did the crime.

---
# Hypothesis Testing

We __reject__ the null hypothesis, but the null is actually true. 

- The defendant was convicted, but he didn't do the crime!
- **Type I error** (a.k.a. _false positive_)

--

We __fail to reject__ the null hypothesis, but the null is actually false.

- The defendant was acquitted, but he did the crime!
- **Type II error** (a.k.a. _false negative_)

---
# Hypothesis Testing

$\hat{\mu}$ is random: it could be anything, even if $\mu = \mu_0$ is true.

- But if $\mu = 0$ is true, then $\hat{\mu}$ is unlikely to take values far from zero.

- As the variance of $\hat{\mu}$ shrinks, we are even less likely to observe "extreme" values of $\hat{\mu}$ (assuming $\mu = \mu_0$).

--

Our test should take extreme values of $\hat{\mu}$ as evidence against the null hypothesis, but it should also weight them by what we know about the variance of $\hat{\mu}$.

- For now, we'll assume that the variable of interest $X$ is normally distributed with mean $\mu$ and standard deviation $\sigma^2$.

---
# Hypothesis Testing

Reject H.sub[0] if $\hat{\mu}$ lies in the .hi[rejection region].

```{r, echo = F, dev = "svg", fig.height = 3.75}
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dnorm(seq(-4,4, by = 0.01))
)
crit <- qnorm(c(.025,.975))
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0), breaks = c(-1.96, 0, 1.96), labels = c(TeX("$\\mu_0 - 1.96 \\, s.d.$"), TeX("$\\mu_0$"), TeX("$\\mu_0 + 1.96 \\, sd$"))) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = df %>% filter(x <= qnorm(1 - 0.975) & x >= qnorm(0.975)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qnorm(0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  geom_vline(xintercept = qnorm(1 - 0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  theme_simple +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- The area of the rejection region is defined by the **significance level** of the test.
- In a 5% test, the area is 0.05. 
- Significance level .mono[=] tolerance for Type I error.

---
# Hypothesis Testing

Reject H.sub[0] if $\left| z \right| =\left| \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{sd}}(\hat{\mu})} \right| > 1.96$.

```{r, echo = F, dev = "svg", fig.height = 3.75}
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dnorm(seq(-4,4, by = 0.01))
)
crit <- qnorm(c(.025,.975))
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0), breaks = c(-1.96, 0, 1.96), labels = c(TeX("$\\mu_0 - 1.96 \\, s.d.$"), TeX("$\\mu_0$"), TeX("$\\mu_0 + 1.96 \\, sd$"))) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = df %>% filter(x <= qnorm(1 - 0.975) & x >= qnorm(0.975)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qnorm(0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  geom_vline(xintercept = qnorm(1 - 0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  theme_simple +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

What happens to $z$ as $\left| \hat{\mu} - \mu_0 \right|$ increases? 

What happens to $z$ as $\mathop{\text{sd}}(\hat{\mu})$ increases?

---
# Hypothesis Testing

The formula for the $z$ statistic assumes that we know $\mathop{\text{sd}}(\hat{\mu})$.

- In practice, we don't know $\mathop{\text{sd}}(\hat{\mu})$, so we have to estimate it.

--

If the variance of $X$ is $\sigma^2$, then 

$$\sigma^2_{\hat{\mu}} = \dfrac{\sigma^2}{n}.$$

- We can estimate $\sigma^2$ with the sample variance $S_{X}^2$.

--

The sample variance of the sample mean is
 
$$S_{\hat{\mu}}^2 = \dfrac{1}{n(n-1)} \sum_{i=1}^n (X_i - \bar{X})^2.$$

---
# Hypothesis Testing

The .hi[standard error] of $\hat{\mu}$ is the square root of $S_{\hat{\mu}}^2$:

$$\mathop{\text{SE}}(\hat{\mu}) = \sqrt{ \dfrac{1}{n(n-1)} \sum_{i=1}^n (X_i - \bar{X})^2}.$$

- Standard error = sample standard deviation of an estimator.

--

When we use $\mathop{\text{SE}}(\hat{\mu})$ in place of $\mathop{\text{sd}}(\hat{\mu})$, the $z$ statistic becomes a $t$ statistic:

$$t = \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{SE}}(\hat{\mu})}.$$

- Unlike the standard deviation of $\hat{\mu}$, $\mathop{\text{SE}}(\hat{\mu})$ varies from sample to sample.
- **Consequence:** $t$ statistics do not necessarily have a normal distribution.

---
# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, dev = "svg", fig.height = 3.5}
n <- 5
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_simple +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 5.

---
count: false

# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, dev = "svg", fig.height = 3.5}
n <- 50
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_simple +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 50.

---
count: false

# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, dev = "svg", fig.height = 3.5}
n <- 500
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_simple +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 500.

---
# Hypothesis Testing

## **t Tests** (two-sided)

To conduct a t test, compare the $t$ statistic to the appropriate .hi[critical value] of the t distribution.

- To find the critical value in a t table, we need the degrees of freedom and the significance level $\alpha$.

Reject H.sub[0] at the $\alpha \cdot 100$-percent level if 

$$\left| t \right| = \left| \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{SE}}(\hat{\mu})} \right| > t_\text{crit}.$$

---
# Hypothesis Testing

## On Your Own

As the term progresses, we will encounter additional flavors of hypothesis testing and other related concepts.

You may find it helpful to review the following topics from Math 243:

- Confidence intervals
- One-sided $t$ tests
- $p$ values

---
class: inverse, middle

# Data and the .mono[tidyverse]

---
# Data

## Experimental data

Data generated in controlled, laboratory settings.

--

Ideal for __causal identification__, but difficult to obtain in the social sciences.

- Intractable logistical problems
- Too expensive
- Morally repugnant

--

Experiments outside the lab: __randomized control trials__ and __A/B testing__.

---
# Data

## Observational data

Data generated in non-experimental settings.

--

- Surveys
- Censuses
- Administrative records
- Environmental data
- Financial and sales transactions
- Social media

--

Mainstay of economic research, but __poses challenges__ to causal identification.

---
# Tidy Data

.more-left[

```{r, echo=FALSE}
data(murders)
murders <- select(murders, state, population, total)
DT::datatable(
  murders,
  colnames = c('<span style="color: #007935 !important">State</span>', '<span style="color: #007935 !important">Population</span>', '<span style="color: #007935 !important">Murders</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, pagingType = "simple"), escape = FALSE) %>%
  DT::formatStyle('state', color = '#9370DB') %>%
  DT::formatStyle('population', color = '#9370DB') %>%
  DT::formatStyle('total', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

]

.less-right[

.hi-orange[Rows] represent .hi-orange[observations].

.hi-green[Columns] represent .hi-green[variables].

Each .hi-purple[value] is associated with an .hi-orange[observation] and a .hi-green[variable].

]

---
# Cross Sectional Data

.hi-purple[Sample of individuals from a population at a point in time.]

Ideally, collected using __random sampling__.

- Random sampling .mono[+] sufficient sample size .mono[=] representative sample.

- Random sampling simplifies data analysis, but non-random samples are common (and difficult to work with).

Used extensively in applied microeconomics.<sup>*</sup>

__Main focus of this course.__

.footnote[
<sup>*</sup> Applied microeconomics .mono[=] Labor, health, education, public finance, development, industrial organization, and urban economics.
]

---
# Cross Sectional Data

```{r, echo=FALSE}
data(wage1)
wage1 <- select(wage1, wage, educ, tenure, female, nonwhite) %>%
  mutate(wage = round(wage, 2))
DT::datatable(
  wage1,
  caption = c("Sample of US workers (Current Population Survey, 1976)"),
  colnames = c('<span style="color: #007935 !important">Wage</span>', '<span style="color: #007935 !important">Education</span>', '<span style="color: #007935 !important">Tenure</span>', '<span style="color: #007935 !important">Female?</span>', '<span style="color: #007935 !important">Non-white?</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('wage', color = '#9370DB') %>%
  DT::formatStyle('educ', color = '#9370DB') %>%
  DT::formatStyle('tenure', color = '#9370DB') %>%
  DT::formatStyle('female', color = '#9370DB') %>%
  DT::formatStyle('nonwhite', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Time Series Data

.hi-purple[Observations of variables over time.]

- Quarterly US GDP
- Annual US infant mortality rates
- Daily Amazon stock prices

Complication: Observations are not independent draws.

- GDP this quarter highly related to GDP last quarter.

Used extensively in empirical macroeconomics.

Requires more-advanced methods (EC 421 and EC 422).

---
# Time Series Data

```{r, echo=FALSE}
data(StrikeNb)
StrikeNb <- select(StrikeNb, time, strikes, output)
DT::datatable(
  StrikeNb,
  caption = c("Number of US manufacturing strikes per month (Jan. 1968 to Dec. 1976)"),
  colnames = c('<span style="color: #007935 !important">Period</span>', '<span style="color: #007935 !important">Strikes</span>', '<span style="color: #007935 !important">Output</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('time', color = '#9370DB') %>%
  DT::formatStyle('strikes', color = '#9370DB') %>%
  DT::formatStyle('output', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Pooled Cross Sectional Data

.hi-purple[Cross sections from different points in time.]

Useful for studying policy changes and relationship that change over time.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).

---
# Pooled Cross Sectional Data

```{r, echo=FALSE}
data('fertil1')
fertil1 <- select(fertil1, year, educ, age, kids, black)
DT::datatable(
  fertil1,
  caption = c("Sample of US women (General Social Survey, 1972 to 1984)"),
  colnames = c('<span style="color: #007935 !important">Year</span>', '<span style="color: #007935 !important">Education</span>', '<span style="color: #007935 !important">Age</span>', '<span style="color: #007935 !important">Children</span>', '<span style="color: #007935 !important">Black?</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('year', color = '#9370DB') %>%
  DT::formatStyle('educ', color = '#9370DB') %>%
  DT::formatStyle('age', color = '#9370DB') %>%
  DT::formatStyle('kids', color = '#9370DB') %>%
  DT::formatStyle('black', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Panel or Longitudinal Data

.hi-purple[Time series for each cross-sectional unit.]

- Example: daily attendance data for a sample of students.

Difficult to collect, but useful for causal identification.

- Can control for _unobserved_ characteristics.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).

---
# Panel or Longitudinal Data

```{r, echo=FALSE}
data(Males)
Males <- select(Males, nr, year, exper, wage, union) %>%
  mutate(wage = round(wage, 2))
DT::datatable(
  Males,
  caption = c("Panel of US workers (National Longitudinal Survey of Youth, 1980 to 1987)"),
  colnames = c('<span style="color: #007935 !important">ID</span>', '<span style="color: #007935 !important">Year</span>', '<span style="color: #007935 !important">Experience</span>', '<span style="color: #007935 !important">log(Wage)</span>', '<span style="color: #007935 !important">Union</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('nr', color = '#9370DB') %>%
  DT::formatStyle('year', color = '#9370DB') %>%
  DT::formatStyle('exper', color = '#9370DB') %>%
  DT::formatStyle('union', color = '#9370DB') %>%
  DT::formatStyle('wage', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Messy Data

**Analysis-ready datasets are rare.** Most data are "messy."

The focus of this class is data analysis, but .hi[data wrangling] is a non-trivial part of a data scientist/analyst's job.

.mono[R] has a suite of packages that facilitate data wrangling. 

- `readr`, `tidyr`, `dplyr`, `ggplot2` .mono[+] others.

- Known collectively as the `tidyverse`.

---
# .mono[tidyverse]

## The [`tidyverse`](https://www.tidyverse.org): A package of packages

`readr`: Functions to import data.

`tidyr`: Functions to reshape messy data.

`dplyr`: Functions to work with data.

`ggplot2`: Functions to visualize data.

---
# Workflow

## Step 1: Load packages with `pacman`

```{r}
library(pacman)
p_load(tidyverse)
```

If the `tidyverse` hasn't already been installed, `p_load` will install it. 

Loading the `tidyverse` automatically loads `readr`, `tidyr`, `dplyr`, `ggplot2`, and a few other packages.

---
# Workflow

## Step 2: Import data with `readr`

```{r, eval=FALSE}
workers <- read_csv("03-example_data.csv")
```

CSV files are a common non-proprietary format for storing tabular data.

The `read_csv` function imports CSV (comma-separated values) files.

- Converts the CSV file to a [`tibble`](https://tibble.tidyverse.org), the `tidyverse` version of a `data.frame`.

---
# Workflow

## Step 3: Reshape data with `tidyr`

Variables are stored in rows instead of columns:

```{r, eval=FALSE}
workers
```

---
# Workflow

## Step 3: Reshape data with `tidyr`

Make the data tidy by using the `spread` function:

```{r, eval=FALSE}
workers <- workers %>% 
  spread(key = variable, value = value)
```

Note the use of the .hi[pipe operator].

- .hi[`%>%`] .mono[=] *"and then."*

- Chains multiple commands together without having to define intermediate objects.

---
# Workflow

## Step 3: Reshape data with `tidyr`

The result:

```{r, eval=FALSE}
workers
```

---
# Workflow

## Step 4: Manipulate data with `dplyr`

Generate new variables with `mutate`:

```{r, eval=FALSE}
workers <- workers %>% 
  mutate(union = ifelse(union == 1, "Yes", "No"))
```

Before, `union` was a binary variable equal to 1 if the worker is in a union or 0 if otherwise.

Now `union` is a character variable.

---
# Workflow

## Step 4: Manipulate data with `dplyr`

The result:

```{r, eval=FALSE}
workers
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, eval=FALSE}
workers %>% 
  ggplot(aes(x = educ, y = earnings)) +
  geom_point()
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

Can also use the `cor` function from `base` .mono[R]:

```{r, eval=FALSE}
cor(workers$educ, workers$earnings)
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, eval=FALSE}
workers %>% 
  ggplot(aes(x = educ, y = earnings, color = union)) +
  geom_point()
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, eval=FALSE}
workers %>% 
  ggplot(aes(x = educ, y = earnings, color = union)) +
  geom_point() +
  facet_grid(~union)
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

Can .hi[subset] the data to get group-specific correlations: 

```{r,eval=FALSE}
workers_union <- workers %>% 
  filter(union == "Yes") #<<

cor(workers_union$educ, workers_union$earnings)
```

```{r,eval=FALSE}
workers_nounion <- workers %>% 
  filter(union == "No") #<<

cor(workers_nounion$educ, workers_nounion$earnings)
```

---
# Why Bother?

**Q:** Why not just use .mono[.hi-green[MS Excel]] for data wrangling?

--

**A:** .hi[Reproducibility]

- Easier to retrace your steps with .mono[R].

--

**A:** .hi[Portability]

- Easy to re-purpose .mono[R] code for new projects.

--

**A:** .hi[Scalability]

- .mono[Excel] chokes on big datasets.

--

**A:** .hi[.mono[R] Saves time] (eventually)

- Lower marginal costs in exchange for higher fixed costs.

---
# Further Reading

1. [Tidy Data](https://vita.had.co.nz/papers/tidy-data.pdf) by Hadley Wickham (creator of the `tidyverse`)

2. [Cheatsheets](https://rstudio.com/resources/cheatsheets/)

---
# The Week Ahead

## Lab 2

Learn `tidyverse` basics.

Tips for the computational section of Problem Set 1.

## Week 3 Reading

Mastering 'Metrics chapters 1 and 2.

Read these chapters before lecture.

## Week 3 Lectures

Goal: Structure our thinking about causality.
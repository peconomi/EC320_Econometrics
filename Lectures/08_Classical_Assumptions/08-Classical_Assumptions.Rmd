---
title: "Classical Assumptions"
subtitle: "EC 320: Introduction to Econometrics"
author: "Philip Economides"
date: "Winter 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, wooldridge, stargazer, latex2exp, parallel, broom, ggforce)
# Define colors
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
met_slate <- "#23373b" # metropolis font color
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  warning = F,
  message = F
)  
theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
# Neumark data
data <- get(data(wage2))
lm_e <- lm(lwage ~ educ, data = data)
b0 <- lm_e$coefficients[1]
b1 <- lm_e$coefficients[2]
r_2 <- summary(lm(lwage ~ educ, data = data))$r.squared
```

# Prologue

---
# Housekeeping

<br>

- Reminder: .hi-pink[Problem Set 3] due by 11:59pm

- This lecture last one relevant to .hi-pink[Midterm exam]

- Revise material, have questions ready for .hi-pink[review session]

- Anything after this lecture will not come up in midterm exam



---
# Agenda

## Last Week

How does OLS estimate a regression line?

- .hi-pink[Minimize RSS].

What are the direct consequences of minimizing RSS?

- Residuals sum to zero. 
- Residuals and the explanatory variable $X$ are uncorrelated.
- Mean values of $X$ and $Y$ are on the fitted regression line.

Whatever do we mean by *goodness of fit*?

- What information does $R^2$ convey?

---
# Agenda

## Today

Under what conditions is OLS *desirable*?

- **Desired properties:** Unbiasedness, efficiency, and ability to conduct hypothesis tests.
- **Cost:** Six .hi-green[classical assumptions] about the population relationship and the sample.

---
# Returns to Schooling

__Policy Question:__ How much should the state subsidize higher education?

- Could higher education subsidies increase future tax revenue?
- Could targeted subsidies reduce income inequality and racial wealth gaps?
- Are there positive externalities associated with higher education? 

--

__Empirical Question:__ What is the monetary return to an additional year of education?

- Focuses on the private benefits of education. Not the only important question!
- Useful for learning about the econometric assumptions that allow causal interpretation. 

---
# Returns to Schooling

**Step 1:** Write down the population model.

$$\log(\text{Earnings}_i) = \beta_1 + \beta_2\text{Education}_i + u_i$$

--

**Step 2:** Find data.

- *Source:* [Blackburn and Neumark (1992)](https://econpapers.repec.org/article/oupqjecon/v_3a107_3ay_3a1992_3ai_3a4_3ap_3a1421-1436..htm).

--

**Step 3:** Run a regression using OLS.

$$\log(\hat{\text{Earnings}_i}) = \hat{\beta}_1 + \hat{\beta}_2\text{Education}_i$$

---
# Returns to Schooling

$\log(\hat{\text{Earnings}_i})$ $=$ .hi-purple[`r round(b0, 2)`] $+$ .hi-purple[`r round(b1, 2)`] $\times$ $\text{Education}_i$.

```{r, echo=FALSE, dev = "svg", fig.height = 5.75}
data %>% 
  ggplot() +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Returns to Schooling

Additional year of school associated with a .hi-purple[`r round(b1, 2)*100`%] increase in earnings. 

```{r, echo=FALSE, dev = "svg", fig.height = 5.75}
data %>% 
  ggplot() +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Returns to Schooling

$R^2$ $=$ .hi-purple[`r round(r_2, 3)`]. 

```{r, echo=FALSE, dev = "svg", fig.height = 5.75}
data %>% 
  ggplot() +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Returns to Schooling

Education explains .hi-purple[`r round(r_2, 3)*100`%] of the variation in wages. 

```{r, echo=FALSE, dev = "svg", fig.height = 5.75}
data %>% 
  ggplot() +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Returns to Schooling

What must we __assume__ to interpret $\hat{\beta}_2$ $=$ .hi-purple[`r round(b1, 2)`] as the return to schooling?

```{r, echo=FALSE, dev = "svg", fig.height = 5.75}
data %>% 
  ggplot() +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Residuals *vs.* Errors

The most important assumptions concern the error term $u_i$.

--

**Important:** An error $u_i$ and a residual $\hat{u}_i$ are related, but different.

--

- .hi-green[Error:] Difference between the wage of a worker with 16 years of education and the .hi-green[expected wage] with 16 years of education.


--

- .hi-purple[Residual:] Difference between the wage of a worker with 16 years of education and the .hi-purple[average wage] of workers with 16 years of education.

--

- .hi-green[Population] ***vs.*** .hi-purple[sample]**.**

---
# Residuals *vs.* Errors

A .hi[residual] tells us how a .hi-slate[worker]'s wages compare to the average wages of workers in the .hi-purple[sample] with the same level of education.

```{r, echo=FALSE, dev = "svg", fig.height = 5.5}
y_hat <- function(x, b0, b1) {b0 + b1 * x}
data %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = "darkslategray", alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = red_pink, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = "#708090", size = 3) +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Residuals *vs.* Errors

A .hi[residual] tells us how a .hi-slate[worker]'s wages compare to the average wages of workers in the .hi-purple[sample] with the same level of education.

```{r, echo=FALSE, dev = "svg", fig.height = 5.5}
y_hat <- function(x, b0, b1) {b0 + b1 * x}
data %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = "darkslategray", alpha = 0.1) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = red_pink, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = "#708090", size = 3) +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Residuals *vs.* Errors

An .hi-orange[error] tells us how a .hi-slate[worker]'s wages compare to the expected wages of workers in the .hi-green[population] with the same level of education.

```{r, echo=FALSE, dev = "svg", fig.height = 5.5}
y_hat <- function(x, b0, b1) {b0 + b1 * x}
B0 <- b0 + 0.5
B1 <- b1 - 0.035
data %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = "darkslategray", alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, B0, B1), color = (7.75 - y_hat(11, B0, B1))^2), linetype = "solid", color = "#FD5F00", size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = "#708090", size = 3) +
  geom_abline(intercept = b0, slope = b1, color = "#9370DB", size = 1) +
  geom_abline(intercept = B0, slope = B1, color = "#007935", size = 1, linetype = "solid") +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
class: inverse, middle

# Classical Assumptions

---
# Classical Assumptions of OLS

1. **Linearity:** The population relationship is .hi[linear in parameters] with an additive error term.

--

2. **Sample Variation:** There is variation in $X$.

--

3. **Exogeneity:** The $X$ variable is .hi[exogenous] (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).<sup>.pink[â€ ]</sup>

--

4. **Homoskedasticity:** The error term has the same variance for each value of the independent variable (*i.e.,* $\mathop{\text{Var}}(u|X) = \sigma^2$).

--

5. **Non-autocorrelation:** The values of error terms have independent distributions (*i.e.,* $E[u_i u_j]=0, \forall i \text{ s.t. } i \neq j$)

--

6. **Normality:** The population error term is normally distributed with mean zero and variance $\sigma^2$ (*i.e.,* $u \sim N(0,\sigma^2)$)

.footnote[
.pink[â€ ] Implies assumption of **Random Sampling:** We have a random sample from the population of interest.
]


---
class: inverse, middle

# When Can We Trust OLS?

---
# Bias

An estimator is __biased__ if its expected value is different from the true population parameter.

.pull-left[

**Unbiased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] = \beta$

```{R, unbiased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

--

.pull-right[

**Biased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

---
# When is OLS Unbiased?

## Required Assumptions

1. **Linearity:** The population relationship is .hi[linear in parameters] with an additive error term.

2. **Sample Variation:** There is variation in $X$.

3. **Exogeneity:** The $X$ variable is .hi[exogenous] (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).

--

&#9755; (3) implies **Random Sampling**. Without, the internal validity of OLS uncompromised, but our external validity becomes uncertain.<sup>.pink[â€ ]</sup>

.footnote[
.pink[â€ ] **Internal Validity:** relates to how well a study is conducted (does it satisfy OLS assumptions?).<br> **External Validity:** relates to how applicable the findings are to the real world.
]

---

## Result

OLS is unbiased.

---
# Linearity (A1.)

## Assumption

The population relationship is __linear in parameters__ with an additive error term.

--

## Examples

- $\text{Wage}_i = \beta_1 + \beta_2 \text{Experience}_i + u_i$

--

- $\log(\text{Happiness}_i) = \beta_1 + \beta_2 \log(\text{Money}_i) + u_i$

--

- $\sqrt{\text{Convictions}_i} = \beta_1 + \beta_2 (\text{Early Childhood Lead Exposure})_i + u_i$

--

- $\log(\text{Earnings}_i) = \beta_1 + \beta_2 \text{Education}_i + u_i$

---
# Linearity (A1.)

## Assumption

The population relationship is __linear in parameters__ with an additive error term.

## Violations

- $\text{Wage}_i = (\beta_1 + \beta_2 \text{Experience}_i)u_i$

--

- $\text{Consumption}_i = \frac{1}{\beta_1 + \beta_2 \text{Income}_i} + u_i$

--

- $\text{Population}_i = \frac{\beta_1}{1 + e^{\beta_2 + \beta_3 \text{Food}_i}} + u_i$

--

- $\text{Batting Average}_i = \beta_1 (\text{Wheaties Consumption})_i^{\beta_2} + u_i$

---
# Sample Variation (A2.)

## Assumption 

There is variation in $X$.

## Example

```{r, echo=FALSE, dev = "svg", fig.height = 4}
data %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Sample Variation (A2.)

## Assumption 

There is variation in $X$.

## Violation

```{r, echo=FALSE, dev = "svg", fig.height = 4}
data %>% 
  filter(educ == 13) %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = "darkslategray") +
  theme_simple + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---
# Exogeneity (A3.)

## Assumption

The $X$ variable is __exogenous:__ $\mathop{\mathbb{E}}\left( u|X \right) = 0$.

- For _any_ value of $X$, the mean of the error term is zero.

.hi[The most important assumption!]

--

Really two assumptions bundled into one:

1. On average, the error term is zero: $\mathop{\mathbb{E}}\left(u\right) = 0$.

2. The mean of the error term is the same for each value of $X$: $\mathop{\mathbb{E}}\left( u|X \right) = \mathop{\mathbb{E}}\left(u\right)$.

---
# Exogeneity (A3.)

## Assumption

The $X$ variable is __exogenous:__ $\mathop{\mathbb{E}}\left( u|X \right) = 0$.

- The assignment of $X$ is effectively random.
- **Implication:** .hi-purple[no selection bias] and .hi-green[no omitted-variable bias].

--

## Examples

In the labor market, an important component of $u$ is unobserved ability.

- $\mathop{\mathbb{E}}\left( u|\text{Education} = 12 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Education} = 20 \right) = 0$.
- $\mathop{\mathbb{E}}\left( u|\text{Experience} = 0 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Experience} = 40 \right) = 0$.
- **Do you believe this?**

---
layout: false
class: white-slide, middle

Graphically...
---
exclude: true

```{R, conditional_expectation_setup, include = F, cache = T}

# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n) + 0.5 * (x - 13.5),
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    theme_pander(base_family = "Fira Sans", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = met_slate),
      axis.title.x = element_text(size = 22, color = met_slate)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    theme_pander(base_family = "Fira Sans", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = met_slate),
      axis.title.x = element_text(size = 22, color = met_slate)
    )
```
---
class: white-slide

Valid exogeneity, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
class: white-slide

Invalid exogeneity, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```

---
class: inverse, middle

# Variance Matters, Too

---
# Why Variance Matters

Unbiasedness tells us that OLS gets it right, _on average_.

- But we can't tell whether our sample is "typical."

--

**Variance** tells us how far OLS can deviate from the population mean.

- How tight is OLS centered on its expected value?

- This determines the .hi-pink[efficiency] of our estimator.

--

The smaller the variance, the closer OLS gets, **on average**, to the true population parameters _on any sample_.

- Given two unbiased estimators, we want the one with smaller variance.

- If (A4.) and (A5.) are satisfied as well, we are using the .hi-pink[most efficient] linear estimator.

---
# OLS Variance

To calculate the variance of OLS, we need:

1. The same four assumptions we made for unbiasedness.

2. __Homoskedasticity.__

3. __Non-autocorrelation__

---

# Homoskedasticity (A4.)

## Assumption

The error term has the same variance for each value of the independent variable:

$$\mathop{\text{Var}}(u|X) = \sigma^2.$$

## Example

```{r, dev = "svg", echo = F, fig.height = 3.25}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "Independent Variable", y = "Error Term") +
theme_simple
```

---
# Homoskedasticity (A4.)

## Assumption

The error term has the same variance for each value of the independent variable:

$$\mathop{\text{Var}}(u|X) = \sigma^2.$$

## Violation: Heteroskedasticity

```{r, dev = "svg", echo = F, fig.height = 3.25}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "Independent Variable", y = "Error Term") +
theme_simple
```

---
count: false

# Homoskedasticity (A4.)

## Assumption

The error term has the same variance for each value of the independent variable:

$$\mathop{\text{Var}}(u|X) = \sigma^2$$

## Violation: Heteroskedasticity

```{r, dev = "svg", echo = F, fig.height = 3.25}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "Independent Variable", y = "Error Term") +
theme_simple
```

---

# Non-Autocorrelation 

## Assumption

Any individual's error term is drawn independently of other error terms. 

$$\mathop{\text{Cov}}(u_i, u_j) = E[(u_i - \mu_u)(u_j - \mu_u)]\\
                                = E[u_i u_j] = E[u_i] E[u_j]  = 0, \text{where } i \neq j$$

- This implies no systematic association between error term values for any pair of individuals

- In practice, there is always some correlatio in unobservables across individuals (e.g. common correlation in unobservables among individuals within a given US state)

- Referred to as .hi-pink[clustering] problem. Standard errors can be adjusted to address


---

# OLS Variance 

Variance of the slope estimator:

$$\mathop{\text{Var}}(\hat{\beta}_2) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}.$$

- As the error variance increases, the variance of the slope estimator increases.

- As the variation in $X$ increases, the variance of the slope estimator decreases.

- Larger sample sizes exhibit more variation in $X \implies \mathop{\text{Var}}(\hat{\beta}_2)$ falls as $n$ rises.

---
class: inverse, middle

# Gauss-Markov

---
# Gauss-Markov Theorem

OLS is the __Best Linear Unbiased Estimator (BLUE)__ when:

1. **Linearity:** The population relationship is .hi[linear in parameters] with an additive error term.

2. **Sample Variation:** There is variation in $X$.

3. **Exogeneity:** The $X$ variable is .hi[exogenous] (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).

4. **Homoskedasticity:** The error term has the same variance for each value of the independent variable (*i.e.,* $\mathop{\text{Var}}(u|X) = \sigma^2$).

5. **Non-Autocorrelation:** Any pair of error terms are drawn independently of eachother (*i.e.,* $\mathop{\text{E}}(u_i u_j) = 0 \ \forall \ i \text{ s.t. } i \neq j$)

---
class: middle

# Gauss-Markov Theorem

OLS is the __Best Linear Unbiased Estimator (BLUE)__

```{R, echo = F, dev = "svg"}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(purple, red, orange)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -1.5,    1.5),
  y  = c( 0.0,   -2.25,   -2.25),
  r  = c( 1.9,    1.9,    1.9),
  l  = c( "Linear", "Unbiased", "Min. Variance"),
  xl = c( 0.0,   -1.5,    1.5),
  yl = c( 0.0,   -2.25,   -2.25)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = F) +
annotate(
  x = -5, y = 2.65,
  geom = "text", label = "Universe of Estimators", size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-5.5, 5.5) +
ylim(-4.5, 2.8) +
coord_equal()
```

---
class: inverse, middle

# Population *vs.* Sample, Revisited

---
layout: true

# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

---

--

```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 1, X = 1:1e3, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Population**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Population relationship**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_1 + \beta_2 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 1:** 30 random individuals]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 2:** 30 random individuals]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 3:** 30 random individuals]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: white-slide, middle

Repeat **10,000 times** (Monte Carlo simulation).

---
layout: true
class: white-slide

---

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- On **average**, the regression lines match the population line nicely.

- However, **individual lines** (samples) can miss the mark.

- Differences between individual samples and the population create **uncertainty**.

]

---


--

**Answer:** Uncertainty matters.

$\hat{\beta}_1$ and $\hat{\beta}_2$ are random variables that depend on the random sample. 

We can't tell if we have a "good" sample (similar to the population) or a "bad sample" (very different than the population).

--

Next time, we will leverage all six classical assumptions, including **normality**, to conduct hypothesis tests.


---

exclude: true

```{R generate pdfs, include = F, eval = F}
#remotes::install_github('rstudio/pagedown')
library(pagedown)
pagedown::chrome_print("08-Classical_Assumptions.html", output = "08-Classical_Assumptions.pdf")
```